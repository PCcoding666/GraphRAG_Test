18:35:19,899 graphrag.config.read_dotenv INFO Loading pipeline .env file
18:35:19,902 graphrag.index.cli INFO using default configuration: {
    "llm": {
        "api_key": "REDACTED, length 6",
        "type": "openai_chat",
        "model": "qwen2:7b",
        "max_tokens": 4000,
        "request_timeout": 3600.0,
        "api_base": "http://127.0.0.1:11434/v1",
        "api_version": null,
        "proxy": null,
        "cognitive_services_endpoint": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 0,
        "requests_per_minute": 0,
        "max_retries": 5,
        "max_retry_wait": 60.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 5
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 10
    },
    "async_mode": "threaded",
    "root_dir": "./ragtest",
    "reporting": {
        "type": "file",
        "base_dir": "output/${timestamp}/reports",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "output/${timestamp}/artifacts",
        "storage_account_blob_url": null
    },
    "cache": {
        "type": "file",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "text",
        "base_dir": "input",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.txt$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": false,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "REDACTED, length 6",
            "type": "openai_chat",
            "model": "nomic-embed-text:latest",
            "max_tokens": 4000,
            "request_timeout": 180.0,
            "api_base": "http://127.0.0.1:11434/v1",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 5,
            "max_retry_wait": 60.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 5
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 10
        },
        "async_mode": "threaded",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": null,
        "strategy": null
    },
    "chunks": {
        "size": 300,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null
    },
    "snapshots": {
        "graphml": false,
        "raw_entities": false,
        "top_level_nodes": false
    },
    "entity_extraction": {
        "llm": {
            "api_key": "REDACTED, length 6",
            "type": "openai_chat",
            "model": "qwen2:7b",
            "max_tokens": 4000,
            "request_timeout": 3600.0,
            "api_base": "http://127.0.0.1:11434/v1",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 5,
            "max_retry_wait": 60.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 5
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 10
        },
        "async_mode": "threaded",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "organization",
            "person",
            "geo",
            "event"
        ],
        "max_gleanings": 0,
        "strategy": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "REDACTED, length 6",
            "type": "openai_chat",
            "model": "qwen2:7b",
            "max_tokens": 4000,
            "request_timeout": 3600.0,
            "api_base": "http://127.0.0.1:11434/v1",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 5,
            "max_retry_wait": 60.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 5
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 10
        },
        "async_mode": "threaded",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "REDACTED, length 6",
            "type": "openai_chat",
            "model": "qwen2:7b",
            "max_tokens": 4000,
            "request_timeout": 3600.0,
            "api_base": "http://127.0.0.1:11434/v1",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 5,
            "max_retry_wait": 60.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 5
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 10
        },
        "async_mode": "threaded",
        "prompt": null,
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "REDACTED, length 6",
            "type": "openai_chat",
            "model": "qwen2:7b",
            "max_tokens": 4000,
            "request_timeout": 3600.0,
            "api_base": "http://127.0.0.1:11434/v1",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 5,
            "max_retry_wait": 60.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 5
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 10
        },
        "async_mode": "threaded",
        "enabled": false,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 0,
        "strategy": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": false
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": []
}
18:35:19,904 graphrag.index.create_pipeline_config INFO skipping workflows 
18:35:19,913 graphrag.index.run INFO Running pipeline
18:35:19,913 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at ragtest/output/20241003-183519/artifacts
18:35:19,913 graphrag.index.input.load_input INFO loading input from root_dir=input
18:35:19,913 graphrag.index.input.load_input INFO using file storage for input
18:35:19,914 graphrag.index.storage.file_pipeline_storage INFO search ragtest/input for files matching .*\.txt$
18:35:19,915 graphrag.index.input.text INFO found text files from input, found [('book.txt', {})]
18:35:19,920 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_extracted_entities', 'create_summarized_entities', 'create_base_entity_graph', 'create_final_entities', 'create_final_nodes', 'create_final_communities', 'join_text_units_to_entity_ids', 'create_final_relationships', 'join_text_units_to_relationship_ids', 'create_final_community_reports', 'create_final_text_units', 'create_base_documents', 'create_final_documents']
18:35:19,920 graphrag.index.run INFO Final # of rows loaded: 1
18:35:20,16 graphrag.index.run INFO Running workflow: create_base_text_units...
18:35:20,16 graphrag.index.run INFO dependencies for create_base_text_units: []
18:35:20,19 datashaper.workflow.workflow INFO executing verb orderby
18:35:20,23 datashaper.workflow.workflow INFO executing verb zip
18:35:20,26 datashaper.workflow.workflow INFO executing verb aggregate_override
18:35:20,33 datashaper.workflow.workflow INFO executing verb chunk
18:35:22,707 datashaper.workflow.workflow INFO executing verb select
18:35:22,711 datashaper.workflow.workflow INFO executing verb unroll
18:35:22,719 datashaper.workflow.workflow INFO executing verb rename
18:35:22,724 datashaper.workflow.workflow INFO executing verb genid
18:35:22,732 datashaper.workflow.workflow INFO executing verb unzip
18:35:22,736 datashaper.workflow.workflow INFO executing verb copy
18:35:22,739 datashaper.workflow.workflow INFO executing verb filter
18:35:22,751 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_text_units.parquet
18:35:22,903 graphrag.index.run INFO Running workflow: create_base_extracted_entities...
18:35:22,903 graphrag.index.run INFO dependencies for create_base_extracted_entities: ['create_base_text_units']
18:35:22,906 graphrag.index.run INFO read table from storage: create_base_text_units.parquet
18:35:22,933 datashaper.workflow.workflow INFO executing verb entity_extract
18:35:22,943 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://127.0.0.1:11434/v1
18:35:22,961 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for qwen2:7b: TPM=0, RPM=0
18:35:22,961 graphrag.index.llm.load_llm INFO create concurrency limiter for qwen2:7b: 5
18:35:38,427 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:35:38,437 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 15.337000000000003. input_tokens=2234, output_tokens=158
18:35:47,352 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:35:47,353 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 24.262. input_tokens=2234, output_tokens=373
18:35:57,731 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:35:57,732 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 34.65199999999999. input_tokens=2234, output_tokens=436
18:36:04,933 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:36:04,934 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 41.84899999999999. input_tokens=2234, output_tokens=309
18:36:12,867 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:36:12,868 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 49.775000000000006. input_tokens=2234, output_tokens=342
18:36:17,431 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:36:17,432 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 38.994. input_tokens=2234, output_tokens=183
18:36:21,336 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:36:21,337 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 33.983000000000004. input_tokens=2234, output_tokens=156
18:36:25,571 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:36:25,571 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 27.839000000000013. input_tokens=2234, output_tokens=177
18:36:32,915 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:36:32,916 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 27.982000000000014. input_tokens=2233, output_tokens=312
18:36:39,941 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:36:39,942 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 27.073999999999984. input_tokens=2234, output_tokens=308
18:36:46,327 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:36:46,328 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 28.89500000000001. input_tokens=2233, output_tokens=270
18:36:53,687 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:36:53,688 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 32.351. input_tokens=2233, output_tokens=315
18:36:59,579 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:36:59,580 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 34.00799999999998. input_tokens=2233, output_tokens=251
18:37:06,310 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:37:06,311 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 33.39399999999998. input_tokens=2234, output_tokens=293
18:37:15,59 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:37:15,59 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 35.11800000000002. input_tokens=2234, output_tokens=390
18:37:21,977 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:37:21,978 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 35.64999999999998. input_tokens=2235, output_tokens=296
18:37:27,31 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:37:27,32 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 33.34299999999999. input_tokens=2234, output_tokens=210
18:37:36,896 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:37:36,897 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 37.316. input_tokens=2234, output_tokens=434
18:37:42,626 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:37:42,627 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 36.316. input_tokens=2234, output_tokens=250
18:37:47,716 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:37:47,717 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 32.65699999999998. input_tokens=2233, output_tokens=214
18:37:53,17 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:37:53,17 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 31.04000000000002. input_tokens=2234, output_tokens=226
18:38:00,5 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:38:00,6 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 32.97300000000001. input_tokens=2234, output_tokens=300
18:38:06,214 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:38:06,215 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 29.317000000000007. input_tokens=2234, output_tokens=272
18:38:14,400 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:38:14,400 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 31.772999999999996. input_tokens=2233, output_tokens=350
18:38:19,585 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:38:19,586 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 31.867999999999995. input_tokens=2234, output_tokens=225
18:38:25,16 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:38:25,16 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 31.998999999999995. input_tokens=2234, output_tokens=233
18:38:33,187 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:38:33,188 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 33.18100000000001. input_tokens=2234, output_tokens=364
18:38:37,522 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:38:37,523 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 31.30899999999997. input_tokens=2234, output_tokens=180
18:38:42,23 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:38:42,24 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 27.624000000000024. input_tokens=2234, output_tokens=191
18:38:47,579 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:38:47,580 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 27.993000000000023. input_tokens=2234, output_tokens=239
18:38:51,896 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:38:51,896 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 26.880000000000024. input_tokens=2233, output_tokens=178
18:38:55,520 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:38:55,521 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 22.333000000000027. input_tokens=2234, output_tokens=148
18:39:02,498 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:39:02,499 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 24.975000000000023. input_tokens=2234, output_tokens=303
18:39:07,136 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:39:07,137 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 25.113. input_tokens=2234, output_tokens=197
18:39:10,605 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:39:10,606 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 23.024999999999977. input_tokens=2233, output_tokens=142
18:39:16,921 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:39:16,922 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 25.024999999999977. input_tokens=2234, output_tokens=275
18:39:25,733 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:39:25,733 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 30.21199999999999. input_tokens=2234, output_tokens=389
18:39:30,254 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:39:30,255 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 27.755999999999972. input_tokens=2234, output_tokens=192
18:39:36,181 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:39:36,182 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 29.043999999999983. input_tokens=2233, output_tokens=254
18:39:41,855 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:39:41,855 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 31.248999999999967. input_tokens=2234, output_tokens=245
18:39:50,54 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:39:50,55 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 33.132000000000005. input_tokens=2234, output_tokens=360
18:39:55,795 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:39:55,796 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 30.061999999999955. input_tokens=2234, output_tokens=247
18:39:59,64 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:39:59,64 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 28.80899999999997. input_tokens=2235, output_tokens=136
18:40:03,649 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:40:03,649 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 27.46700000000004. input_tokens=2234, output_tokens=195
18:40:09,321 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:40:09,322 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 27.466000000000008. input_tokens=2234, output_tokens=246
18:40:13,723 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:40:13,724 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 23.66900000000004. input_tokens=2234, output_tokens=189
18:40:19,787 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:40:19,788 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 23.990999999999985. input_tokens=2234, output_tokens=263
18:40:25,888 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:40:25,889 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 26.824000000000012. input_tokens=2235, output_tokens=270
18:40:30,595 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:40:30,596 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 26.94599999999997. input_tokens=2233, output_tokens=200
18:40:36,605 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:40:36,606 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 27.28299999999996. input_tokens=2234, output_tokens=262
18:40:42,917 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:40:42,917 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 29.19399999999996. input_tokens=2234, output_tokens=279
18:40:47,311 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:40:47,312 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 27.524. input_tokens=2234, output_tokens=187
18:40:51,321 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:40:51,321 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 25.43199999999996. input_tokens=2234, output_tokens=170
18:40:55,246 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:40:55,247 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 24.649999999999977. input_tokens=2234, output_tokens=165
18:41:00,971 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:41:00,972 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 24.365999999999985. input_tokens=2233, output_tokens=250
18:41:09,100 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:41:09,101 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 26.184000000000026. input_tokens=2233, output_tokens=360
18:41:15,844 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:41:15,845 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 28.533000000000015. input_tokens=2234, output_tokens=295
18:41:19,470 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:41:19,470 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 28.149. input_tokens=2234, output_tokens=152
18:41:25,414 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:41:25,415 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 30.168000000000006. input_tokens=2234, output_tokens=258
18:41:33,294 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:41:33,295 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 32.32299999999998. input_tokens=2234, output_tokens=344
18:41:40,612 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:41:40,612 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 31.510999999999967. input_tokens=2234, output_tokens=317
18:41:48,175 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:41:48,176 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 32.33000000000004. input_tokens=2234, output_tokens=330
18:41:54,271 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:41:54,272 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 34.801000000000045. input_tokens=2234, output_tokens=268
18:42:01,802 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:42:01,803 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 36.387. input_tokens=2234, output_tokens=332
18:42:09,267 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:42:09,267 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 35.97200000000004. input_tokens=2234, output_tokens=330
18:42:15,597 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:42:15,598 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 34.985000000000014. input_tokens=2234, output_tokens=278
18:42:20,7 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:42:20,8 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 31.831000000000017. input_tokens=2234, output_tokens=186
18:42:26,67 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:42:26,68 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 31.795999999999992. input_tokens=2234, output_tokens=263
18:42:30,468 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:42:30,469 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 28.665999999999997. input_tokens=2234, output_tokens=187
18:42:33,305 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:42:33,306 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 24.03800000000001. input_tokens=2234, output_tokens=113
18:42:37,301 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:42:37,302 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 21.702999999999975. input_tokens=2233, output_tokens=169
18:42:41,772 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:42:41,773 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 21.76400000000001. input_tokens=2233, output_tokens=189
18:42:46,525 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:42:46,526 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 20.456999999999994. input_tokens=2233, output_tokens=203
18:42:51,449 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:42:51,450 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 20.980000000000018. input_tokens=2234, output_tokens=206
18:42:58,618 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:42:58,618 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 25.312000000000012. input_tokens=2234, output_tokens=314
18:43:09,835 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:43:09,836 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 32.533000000000015. input_tokens=2234, output_tokens=493
18:43:17,808 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:43:17,809 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 36.035999999999945. input_tokens=2234, output_tokens=350
18:43:23,595 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:43:23,596 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 37.06899999999996. input_tokens=2233, output_tokens=252
18:43:30,798 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:43:30,799 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 39.34899999999993. input_tokens=2233, output_tokens=316
18:43:36,222 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:43:36,223 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 37.60400000000004. input_tokens=2234, output_tokens=232
18:43:41,199 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:43:41,200 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 31.363000000000056. input_tokens=2233, output_tokens=213
18:43:46,591 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:43:46,592 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 28.783000000000015. input_tokens=2235, output_tokens=229
18:43:52,241 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:43:52,242 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 28.646000000000072. input_tokens=2234, output_tokens=237
18:44:03,499 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:44:03,500 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 32.69999999999993. input_tokens=2233, output_tokens=479
18:44:09,302 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:44:09,303 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 33.07899999999995. input_tokens=2234, output_tokens=241
18:44:14,362 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:44:14,363 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 33.16300000000001. input_tokens=2234, output_tokens=210
18:44:21,228 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:44:21,229 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 34.636999999999944. input_tokens=2234, output_tokens=292
18:44:25,729 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:44:25,730 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 33.486999999999966. input_tokens=2234, output_tokens=185
18:44:31,762 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:44:31,763 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 28.262000000000057. input_tokens=2231, output_tokens=248
18:44:37,624 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:44:37,625 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 28.321000000000026. input_tokens=2234, output_tokens=242
18:44:44,478 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:44:44,479 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 30.11500000000001. input_tokens=2234, output_tokens=282
18:44:48,92 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:44:48,93 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 26.863000000000056. input_tokens=2234, output_tokens=140
18:44:54,258 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:44:54,259 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 28.52800000000002. input_tokens=2234, output_tokens=255
18:45:00,139 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:45:00,140 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 28.375999999999976. input_tokens=2234, output_tokens=242
18:45:05,71 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:45:05,71 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 27.447000000000003. input_tokens=2233, output_tokens=200
18:45:09,51 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:45:09,52 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 24.57299999999998. input_tokens=2234, output_tokens=163
18:45:16,46 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:45:16,47 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 27.95399999999995. input_tokens=2234, output_tokens=298
18:45:20,707 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:45:20,708 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 26.44799999999998. input_tokens=2234, output_tokens=195
18:45:30,420 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:45:30,421 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 30.279999999999973. input_tokens=2233, output_tokens=415
18:45:34,66 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:45:34,67 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 28.995000000000005. input_tokens=2234, output_tokens=146
18:45:41,494 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:45:41,495 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 32.44200000000001. input_tokens=2234, output_tokens=309
18:45:46,189 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:45:46,190 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 30.142000000000053. input_tokens=2234, output_tokens=191
18:45:52,756 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:45:52,757 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 32.04900000000009. input_tokens=2234, output_tokens=276
18:45:59,309 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:45:59,310 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 28.888000000000034. input_tokens=2233, output_tokens=278
18:46:03,635 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:46:03,636 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 29.569000000000074. input_tokens=2233, output_tokens=181
18:46:06,793 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:46:06,794 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 25.298000000000002. input_tokens=2234, output_tokens=128
18:46:10,258 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:46:10,259 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 24.067999999999984. input_tokens=2234, output_tokens=144
18:46:14,220 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:46:14,221 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 21.464999999999918. input_tokens=2234, output_tokens=166
18:46:22,109 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:46:22,110 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 22.799999999999955. input_tokens=2234, output_tokens=347
18:46:26,391 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:46:26,392 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 22.754999999999995. input_tokens=2234, output_tokens=180
18:46:29,817 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:46:29,818 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 23.024. input_tokens=2234, output_tokens=138
18:46:33,560 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:46:33,561 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 23.301000000000045. input_tokens=2233, output_tokens=153
18:46:41,60 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:46:41,61 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 26.839000000000055. input_tokens=2233, output_tokens=325
18:46:49,451 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:46:49,452 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 27.341000000000008. input_tokens=2234, output_tokens=353
18:46:54,318 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:46:54,319 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 27.926000000000045. input_tokens=2234, output_tokens=199
18:46:59,181 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:46:59,182 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 29.364000000000033. input_tokens=2234, output_tokens=202
18:47:04,5 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:47:04,6 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 30.44399999999996. input_tokens=2234, output_tokens=202
18:47:09,820 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:47:09,820 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 28.759000000000015. input_tokens=2234, output_tokens=244
18:47:18,828 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:47:18,829 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 29.377000000000066. input_tokens=2232, output_tokens=394
18:47:26,864 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:47:26,865 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 32.545999999999935. input_tokens=2234, output_tokens=350
18:47:32,740 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:47:32,741 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 33.55799999999999. input_tokens=2234, output_tokens=249
18:47:39,883 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:47:39,884 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 35.87699999999995. input_tokens=2232, output_tokens=302
18:47:45,502 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:47:45,503 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 35.682000000000016. input_tokens=2234, output_tokens=242
18:47:49,142 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:47:49,143 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 30.313999999999965. input_tokens=2234, output_tokens=148
18:47:54,3 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:47:54,4 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 27.138000000000034. input_tokens=2088, output_tokens=214
18:47:54,37 datashaper.workflow.workflow INFO executing verb merge_graphs
18:47:54,120 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_extracted_entities.parquet
18:47:54,783 graphrag.index.run INFO Running workflow: create_summarized_entities...
18:47:54,783 graphrag.index.run INFO dependencies for create_summarized_entities: ['create_base_extracted_entities']
18:47:54,783 graphrag.index.run INFO read table from storage: create_base_extracted_entities.parquet
18:47:54,807 datashaper.workflow.workflow INFO executing verb summarize_descriptions
18:47:57,571 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:47:57,572 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.745999999999981. input_tokens=296, output_tokens=105
18:47:59,117 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:47:59,118 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.290000000000077. input_tokens=177, output_tokens=66
18:48:01,649 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:48:01,650 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 6.826000000000022. input_tokens=341, output_tokens=101
18:48:02,885 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:48:02,886 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 8.055999999999926. input_tokens=175, output_tokens=50
18:48:05,225 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:48:05,225 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 10.40300000000002. input_tokens=303, output_tokens=94
18:48:06,293 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:48:06,294 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 8.722000000000094. input_tokens=173, output_tokens=43
18:48:16,219 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:48:16,220 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 17.101. input_tokens=3753, output_tokens=360
18:48:20,965 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:48:20,966 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 19.31499999999994. input_tokens=169, output_tokens=209
18:48:28,123 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:48:28,124 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 25.236999999999966. input_tokens=224, output_tokens=323
18:48:29,492 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:48:29,493 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 24.267000000000053. input_tokens=175, output_tokens=54
18:48:31,950 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:48:31,951 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 25.65599999999995. input_tokens=170, output_tokens=108
18:48:36,847 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:48:36,847 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 20.627000000000066. input_tokens=174, output_tokens=218
18:48:38,169 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:48:38,170 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 17.204000000000065. input_tokens=190, output_tokens=54
18:48:40,133 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:48:40,134 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 12.009999999999991. input_tokens=172, output_tokens=83
18:48:42,933 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:48:42,934 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 13.43999999999994. input_tokens=199, output_tokens=122
18:48:44,277 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:48:44,277 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 12.326000000000022. input_tokens=172, output_tokens=55
18:48:47,364 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:48:47,364 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 10.516999999999939. input_tokens=233, output_tokens=131
18:48:48,770 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:48:48,771 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 10.600000000000023. input_tokens=178, output_tokens=57
18:48:49,557 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:48:49,558 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 9.423999999999978. input_tokens=171, output_tokens=28
18:48:51,40 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:48:51,40 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 8.107000000000085. input_tokens=174, output_tokens=60
18:48:52,823 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:48:52,824 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 8.54600000000005. input_tokens=206, output_tokens=74
18:48:58,130 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:48:58,131 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 10.765999999999963. input_tokens=437, output_tokens=216
18:49:01,318 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:49:01,318 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 12.548000000000002. input_tokens=280, output_tokens=132
18:49:04,146 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:49:04,147 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 14.58800000000008. input_tokens=179, output_tokens=120
18:49:06,823 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:49:06,823 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 15.783000000000015. input_tokens=231, output_tokens=111
18:49:12,55 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:49:12,56 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 19.230999999999995. input_tokens=513, output_tokens=215
18:49:15,399 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:49:15,400 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 17.269000000000005. input_tokens=223, output_tokens=145
18:49:18,61 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:49:18,62 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 16.742999999999938. input_tokens=170, output_tokens=112
18:49:19,413 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:49:19,414 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 15.266999999999939. input_tokens=165, output_tokens=56
18:49:20,622 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:49:20,623 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 13.798999999999978. input_tokens=161, output_tokens=51
18:49:23,807 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:49:23,808 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 11.751999999999953. input_tokens=209, output_tokens=137
18:49:26,383 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:49:26,384 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 10.982999999999947. input_tokens=245, output_tokens=104
18:49:30,854 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:49:30,854 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 12.79200000000003. input_tokens=172, output_tokens=194
18:49:36,498 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:49:36,499 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 17.08400000000006. input_tokens=215, output_tokens=245
18:49:37,556 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:49:37,556 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 16.93399999999997. input_tokens=171, output_tokens=42
18:49:38,766 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:49:38,766 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 14.958000000000084. input_tokens=165, output_tokens=49
18:49:44,423 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:49:44,424 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 18.039999999999964. input_tokens=179, output_tokens=255
18:49:45,919 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:49:45,920 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 15.065000000000055. input_tokens=175, output_tokens=58
18:49:50,21 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:49:50,22 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 13.522000000000048. input_tokens=199, output_tokens=179
18:49:51,900 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:49:51,900 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 14.344000000000051. input_tokens=191, output_tokens=77
18:49:56,43 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:49:56,43 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 17.27699999999993. input_tokens=359, output_tokens=171
18:50:00,946 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:50:00,947 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 16.521999999999935. input_tokens=201, output_tokens=212
18:50:03,74 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:50:03,75 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 17.154999999999973. input_tokens=219, output_tokens=87
18:50:05,510 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:50:05,511 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 15.487999999999943. input_tokens=270, output_tokens=101
18:50:07,362 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:50:07,363 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 15.462999999999965. input_tokens=183, output_tokens=76
18:50:09,773 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:50:09,774 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 13.730000000000018. input_tokens=226, output_tokens=104
18:50:13,33 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:50:13,34 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 12.086000000000013. input_tokens=231, output_tokens=142
18:50:16,617 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:50:16,618 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 13.543000000000006. input_tokens=174, output_tokens=159
18:50:19,802 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:50:19,803 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 14.29099999999994. input_tokens=210, output_tokens=133
18:50:22,4 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:50:22,5 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 14.640999999999963. input_tokens=204, output_tokens=95
18:50:23,924 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:50:23,925 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 14.150000000000091. input_tokens=184, output_tokens=80
18:50:25,676 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:50:25,677 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 12.642000000000053. input_tokens=160, output_tokens=73
18:50:27,96 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:50:27,97 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 10.477999999999952. input_tokens=162, output_tokens=58
18:50:29,294 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:50:29,295 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 9.4910000000001. input_tokens=185, output_tokens=91
18:50:31,471 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:50:31,472 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 9.466000000000008. input_tokens=305, output_tokens=89
18:50:33,989 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:50:33,989 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 10.06499999999994. input_tokens=225, output_tokens=105
18:50:35,650 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:50:35,651 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 9.97300000000007. input_tokens=206, output_tokens=67
18:50:37,47 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:50:37,48 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 9.951000000000022. input_tokens=185, output_tokens=56
18:50:39,826 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:50:39,827 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 10.530999999999949. input_tokens=180, output_tokens=120
18:50:41,210 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:50:41,211 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 9.737999999999943. input_tokens=178, output_tokens=59
18:50:42,892 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:50:42,893 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 8.90300000000002. input_tokens=171, output_tokens=75
18:50:44,142 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:50:44,143 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 8.491999999999962. input_tokens=178, output_tokens=52
18:50:45,660 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:50:45,660 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 8.61200000000008. input_tokens=204, output_tokens=64
18:50:53,928 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:50:53,929 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 14.100999999999999. input_tokens=275, output_tokens=377
18:50:58,411 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:50:58,411 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 17.200000000000045. input_tokens=273, output_tokens=201
18:50:59,991 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:50:59,992 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 17.09800000000007. input_tokens=178, output_tokens=65
18:51:02,345 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:51:02,346 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 18.201999999999998. input_tokens=183, output_tokens=103
18:51:09,411 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:51:09,412 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 23.750999999999976. input_tokens=369, output_tokens=320
18:51:10,158 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:51:10,159 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 16.228999999999928. input_tokens=170, output_tokens=30
18:51:16,706 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:51:16,707 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 18.295999999999935. input_tokens=181, output_tokens=301
18:51:19,611 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 200 OK"
18:51:19,612 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 19.618999999999915. input_tokens=179, output_tokens=131
18:51:19,622 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_summarized_entities.parquet
18:51:19,755 graphrag.index.run INFO Running workflow: create_base_entity_graph...
18:51:19,755 graphrag.index.run INFO dependencies for create_base_entity_graph: ['create_summarized_entities']
18:51:19,755 graphrag.index.run INFO read table from storage: create_summarized_entities.parquet
18:51:19,766 datashaper.workflow.workflow INFO executing verb cluster_graph
18:51:19,820 datashaper.workflow.workflow INFO executing verb select
18:51:19,822 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_entity_graph.parquet
18:51:19,931 graphrag.index.run INFO Running workflow: create_final_entities...
18:51:19,932 graphrag.index.run INFO dependencies for create_final_entities: ['create_base_entity_graph']
18:51:19,932 graphrag.index.run INFO read table from storage: create_base_entity_graph.parquet
18:51:19,946 datashaper.workflow.workflow INFO executing verb unpack_graph
18:51:19,967 datashaper.workflow.workflow INFO executing verb rename
18:51:19,971 datashaper.workflow.workflow INFO executing verb select
18:51:19,976 datashaper.workflow.workflow INFO executing verb dedupe
18:51:19,984 datashaper.workflow.workflow INFO executing verb rename
18:51:19,989 datashaper.workflow.workflow INFO executing verb filter
18:51:20,8 datashaper.workflow.workflow INFO executing verb text_split
18:51:20,15 datashaper.workflow.workflow INFO executing verb drop
18:51:20,22 datashaper.workflow.workflow INFO executing verb merge
18:51:20,55 datashaper.workflow.workflow INFO executing verb text_embed
18:51:20,59 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://127.0.0.1:11434/v1
18:51:20,78 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for nomic-embed-text:latest: TPM=0, RPM=0
18:51:20,78 graphrag.index.llm.load_llm INFO create concurrency limiter for nomic-embed-text:latest: 5
18:51:20,111 graphrag.index.verbs.text.embed.strategies.openai INFO embedding 298 inputs via 298 snippets using 19 batches. max_batch_size=16, max_tokens=8191
18:51:20,129 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 400 Bad Request"
18:51:20,130 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 400 Bad Request"
18:51:20,130 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 400 Bad Request"
18:51:20,130 httpx INFO HTTP Request: POST http://127.0.0.1:11434/v1/chat/completions "HTTP/1.1 400 Bad Request"
18:51:20,133 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': ['"BLIND MEN\'S DOGS":"The blind men\'s dogs show awareness of Scrooge and avoid him by guiding their owners away, suggesting an aura of negativity around him."', '"NEPHEW":"The Nephew, portrayed as an individual seeking friendship with Scrooge, faces initial resistance from him. Despite this, he persistently advocates for maintaining a positive perspective towards Christmas and encourages Scrooge to embrace the spirit of the holiday in his own unique manner."', '"MERRY CHRISTMAS":"Merry Christmas" is a phrase associated with contrasting sentiments for Scrooge. On one hand, it\'s linked negatively to financial responsibilities and the sense of aging without accumulating wealth, reflecting his personal concerns and possibly past experiences. On the other hand, the celebration of Merry Christmas is depicted through imagery of sharing grog and singing songs, underscoring the human connection that can be found even in solitude or isolation. This dual nature of "Merry Christmas" encapsulates both the personal struggles and the universal warmth associated with this festive greeting.', '"CHRISTMAS-TIME":"Christmas-time is described as a good time characterized by kindness, forgiveness, charity and warmth.")  ("entity"', '"CLERK":The description provided pertains to "The Clerk", an individual employed by Scrooge. The first description states that this person earns fifteen shillings a week and has a family, while also being greeted cordially by Scrooge. This suggests that the relationship between Scrooge and his clerk is amicable despite the potentially challenging work environment.\n\nThe second description portrays "The Clerk" as an employee who works under Scrooge and is involved in shutting down the office alongside him. This implies a collaborative aspect to their working relationship, where they share responsibilities beyond just daily operations.\n\nIn summary, "The Clerk", depicted as an employee of Scrooge, earns fifteen shillings per week with familial obligations. Despite potentially demanding work conditions, there exists a cordial greeting from Scrooge towards him. Furthermore, this individual is involved in the process of shutting down the office alongside Scrooge, indicating shared responsibilities and perhaps a collaborative working relationship.\n\nPlease note that these descriptions might be taken out of context or may not fully represent the character\'s complete profile within their respective narratives.', '"BEDLAM":"\\"BEDLAM\\" refers to an asylum or institution for the mentally ill, which Scrooge associates with a state of mind akin to insanity. Simultaneously, it symbolizes his perception of others as lacking human qualities, particularly those who are less fortunate."', '"HIS CLERK":"His clerk is someone who works for Scrooge, discussing merry Christmas despite his low salary."', '"SCROOGE AND MARLEY\'S":"Scrooge and Marley\'s is the business owned by Scrooge and his partner Marley."', '"MR. SCROOGE":"Mr. Scrooge primarily refers to Ebenezer Scrooge, the central character in Charles Dickens\' classic novel \'A Christmas Carol.\' He is addressed directly by this term when speaking or referring to him. However, there\'s also a nuanced interpretation where Mr. Scrooge symbolizes an individual who has lost his partner and now finds himself alone, which brings distress to another character in the narrative."', '"MR. MARLEY":"Mr. Marley is the deceased partner of Scrooge in their business, mentioned by one of the gentlemen."', '"SEVEN YEARS AGO, THIS VERY NIGHT":"This event refers to the death of Mr. Marley seven years prior on that same date."', '"THE GENTLEMAN":"The Gentleman" refers to a person who is initially taken by surprise when approached by Scrooge. He agrees to see Scrooge despite this initial shock, demonstrating flexibility and openness. Additionally, "The Gentleman" embodies the role of a representative for various charitable organizations that are dedicated to advocating for the welfare of the poor and destitute. His involvement in charitable activities is further highlighted through his participation in fundraising efforts during Christmas, indicating his commitment to supporting those in need financially. In essence, "The Gentleman" combines elements of personal openness with a strong dedication to social responsibility and philanthropy.', '"THE TREADMILL AND THE POOR LAW":"These are two establishments that Scrooge is aware of being operational, suggesting they are part of a larger system he supports."', '"THE ESTABLISHMENTS":', '"GENTLEMEN":"The \'gentlemen\', referring to a group of individuals, approach Scrooge with inquiries regarding his aspirations for Christmas and his participation in philanthropic endeavors. Subsequently, these same \'gentlemen\' manage to convince Scrooge to abandon his pursuit of a certain objective."', '"CHRISTMAS":"Christmas represents a transformative period for Scrooge, marking a shift in his heart where he decides to honor this event throughout the year, reflecting on his life\'s change. It is also the time when Scrooge responds to questions about festivities and charity, highlighting its significance during this season. The essence of Christmas is further emphasized by the spirits visiting Scrooge, leading to a profound alteration in his perspective. This festive season is symbolized by the presence of Christmas toys and presents, indicating a celebration of joy and merriment. Additionally, Christmas is portrayed as a time for reunion, suggesting a joyful gathering that lasts throughout the entire festive period."']}
18:51:20,134 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': ['"CHARITY":"Charity refers to Scrooge\'s stance on supporting establishments for those who are badly off, indicating his views on social responsibility."', '"CHURCH TOWER":"The ancient tower of a church is described as having a Gothic window and striking hours and quarters in clouds, symbolizing its age and grandeur."', '"LABOURERS":"The labourers are repairing gas-pipes on the main street corner."', '"RAGGED MEN AND BOYS":"A group consisting of ragged men and boys is depicted as being gathered around a fire, engaging in activities such as warming their hands and benefiting from its warmth. This scene suggests a sense of communal gathering or belonging, possibly indicative of an organization or community that these individuals are part of."', '"WATER-PLUG":"The water-plug is left in solitude, causing its overflowings to suddenly congeal and turn into ice."', '"POULTERERS\' AND GROCERS\':"These trades are described as becoming bright with holly sprigs and berries crackling in the lamp heat of their windows."', '"COUNTING-HOUSE":"A counting-house is a location where financial transactions take place, serving as the workplace for Scrooge and being integral to his business operations."', '"EXPECTANT CLERK":"The expectant clerk is someone who anticipates the end of work for the day and has to perform tasks related to shutting down operations."', '"THE KEY":"The key Scrooge has relinquished and then takes back indicates access control or security."', '"THE DOOR":"The door that Scrooge considers opening, but pauses before doing so due to a moment\'s irresolution."', '"MARLEY\'S PIGTAIL":"Marley\'s pigtail is mentioned as something that might stick out of the hall, suggesting an element of surprise or fear."', '"THE HOUSE":"The house where Scrooge resides and experiences echoes when he closes the door."', '"EVERY ROOM ABOVE":"Scrooge mentions every room above as experiencing separate peals of echoes, indicating a large or multi-level dwelling."', '"EVERY CASK IN THE WINE-MERCHANT\'S CELLARS BELOW":"The wine merchant\'s cellars are described as being below Scrooge\'s house and also experiencing echoes when he closes the door."', '"HALL":"The hall is the location where Scrooge walks up stairs and enters his rooms."', '"STAIRS":"Scrooge walks up the stairs, suggesting he\'s moving from one floor to another."']}
18:51:20,134 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': ['"GAS-LAMPS":"The gas-lamps are a source of light that illuminates poorly, creating a dark environment for Scrooge."', '"ENTRY":"Scrooge is in the entryway or foyer when he notices darkness and the dip of his candle."', '"SITTING-ROOM":"The Sitting-room is a specific location in Scrooge\'s house where he conducts his nightly routine."', '"BEDROOM":"Scrooge\'s Bedroom serves as both his final destination and a point of concern after using the extinguisher-cap, symbolizing his return to his physical space. Additionally, it is also an important location in Scrooge\'s house where he meticulously checks for any unusual anomalies before retiring for bed."', '"LUMBER-ROOM":"The Lumber-room is a third specific location in Scrooge\'s house where he ensures everything is as it should be."', '"SAUCEPAN OF GRUEL":"A saucepan containing gruel, which Scrooge has due to having a cold."', '"COLD":"Scrooge\'s condition that necessitates the consumption of gruel."', '"DRESSING-GOWN":"A dressing-gown is an item of clothing that Scrooge puts on, suggesting he\'s preparing for bed or relaxation."', '"NIGHTCAP":"A nightcap is a type of headwear worn by Scrooge before going to sleep, indicating nighttime activities."', '"FIREPLACE":"The fireplace is an old Dutch merchant\'s creation that Scrooge uses for warmth on a cold night, highlighting the setting and its historical element."', '"GRUEL":"Gruel is the food Scrooge consumes, suggesting his diet or lifestyle."', '"MARLEY\'S GHOST":"MARLEY\'S GHOST is a manifestation that embodies the spirit of Jacob Marley, Scrooge\'s late business partner. This ghostly figure appears before Scrooge, carrying forward the essence and influence of Marley in the present. It serves as both a haunting presence and an anxiety-inducing force, reflecting on Scrooge\'s past actions and decisions."', '"HAUNTED HOUSE":"The location where ghosts are said to appear and haunt, in this context referring metaphorically to Scrooge\'s skepticism."', '"GHOST":"GHOST" is a supernatural entity that has appeared before Scrooge, claiming to be his late business partner, Jacob Marley. This ghostly figure introduces a mystical element into the narrative and appears alongside Scrooge in a somber room filled with plain deal forms and desks. The Ghost influences events and decisions in Scrooge\'s life, appearing without warning and impacting his environment, suggesting it has some kind of supernatural or magical influence. It represents a guiding force or spirit that provides moral guidance to someone. This entity from the past expresses regret about business and welfare, emphasizing their importance. The Ghost brings significant change to Scrooge\'s perspective and behavior, visiting him to question his beliefs and guide him through interactions with the unseen world. Representing Jacob Marley\'s spirit, it conveys messages to Scrooge, guiding him towards reflection on his past, present, and future experiences. Ultimately, this entity seems to be an unseen force that walks among humans, influencing Scrooge\'s thoughts and actions, guiding him in his journey of self-discovery and redemption.', '"JACOB MARLEY":"Jacob Marley is mentioned as being Scrooge\'s partner in life.")  ("entity"', '"SPIRITUAL EXISTENCE":"The concept of spirits walking the earth is introduced through the interaction between Scrooge and the Ghost."']}
18:51:20,135 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': ['"PROJECT GUTENBERG":"Project Gutenberg is an organization dedicated to distributing eBooks, with a focus on providing access to public domain books and texts. It operates under certain conditions concerning copyright status and requires users to check local laws before utilizing its materials. The organization produces and distributes free eBooks with the support of volunteers and promotes free access to electronic works, subject to specific terms of use and intellectual property agreements. Project Gutenberg also provides electronic works that are available for distribution without copyright royalties in the United States."', '"THE PROJECT GUTENBERG EBOOK A CHRISTMAS CAROL":"A Christmas Carol is a book produced by Project Gutenberg, indicating its digital availability and wide accessibility.") ("entity"', '"1958":"The year 1958 is mentioned without context, potentially marking the beginning or an event in a series of years." ) ("entity"', '"MR. FEZZIWIG":"Mr. Fezziwig is a kind-hearted, jovial old merchant.") ("entity"', '"MRS. CRATCHIT":"MRS. CRATCHIT is the protagonist of this scene and the matriarch of the Cratchit family from Charles Dickens\' \'A Christmas Carol\'. She is responsible for making gravy and addressing her children, indicating she\'s likely a housewife or caretaker in the household. Mrs. Cratchit warmly welcomes her family back home on Christmas Day and shows affection by kissing little Bob. She is concerned about her children being late for Christmas activities and dresses bravely in ribbons despite dressing poorly."', '"BOB CRATCHIT":"Bob Cratchit is a character known for expressing happiness and concern for Tiny Tim\'s memory. He also calmly states his opinion about the pudding, praising Mrs. Cratchit for her culinary skills and indicating his admiration and support. Despite his low income, Bob receives blessings from the Ghost of Christmas Present on Christmas Eve. As the father of Tiny Tim, he returns from church with his son anticipating their festive celebration. In a memorable scene, Bob is depicted sliding down a slide on Cornhill in honor of Christmas Eve."', '"JOE":"Joe is a character known for his interaction skills as he engages in conversations and activities with other individuals. He poses questions about taking down bed-curtains while someone is lying there, showcasing his curious nature. Moreover, Joe embodies the role of an individual who not only speaks but also handles the possessions of others during these interactions."', '"STAVE ONE":"STAVE ONE refers to the initial segment or chapter within a narrative or book, typically identified by its heading. In the context provided, STAVE ONE specifically denotes the commencement of Marley\'s Ghost, signifying the first part of the story."', '"MARLEY":"MARLEY refers to a deceased partner of Scrooge, whose demise holds considerable significance in their business relationship. The statement \'Marley was dead, to begin with\' confirms that Marley has passed away."', '"SCROOGE":Scrooge is a character who ponders on what the Ghost had said and questions the speed of the Ghost\'s travel.\nScrooge is a character who predicts that it would be necessary for him and the clerk to part.\nScrooge is a character who raises the salary of his employee and promises to assist his struggling family during Christmas.\nScrooge is a character who receives visits from ghosts and experiences changes in his attitude towards life and generosity.\nScrooge is a character who recognizes places from his past and feels joy upon seeing them.\nScrooge is a character who refuses to be friendly with his nephew despite the latter\'s attempts to reconcile.\nScrooge is a character who refuses to believe in ghosts until one appears before him.\nScrooge is a character who refuses to listen or be friendly towards others.\nScrooge is a character who requests payment from another person, showing his business nature.\nScrooge is a character who seeks guidance from an unseen spirit, expressing his willingness to undergo change and improve.\nScrooge is a character who regards everyone with a delighted smile and greets them warmly, despite his past reputation.\nScrooge is a character who significantly improves his behavior and attitude towards others, becoming kinder and more generous.\nScrooge is a character who seeks refuge or resource after being confronted with Doom.\nScrooge is a character who struggles with his thoughts about making choices based on gain or love.\nScrooge is a character who undergoes a transformation after being visited by the Spirit, experiencing horror at the sight of monstrous humans.\nScrooge is a character who undergoes a transformation after being visited by the Spirit, becoming more compassionate and understanding.\nScrooge is a main character who experiences encounters with ghosts and struggles to overcome them.', '"SOLE EXECUTOR", "SOLE ADMINISTRATOR", "SOLE ADMINISTRATOR":"These terms refer to Scrooge\'s role in managing Marley\'s affairs after his death, indicating that he had a significant organizational or administrative responsibility."', '"FUNERAL":"The funeral of Marley marks the end of his life and impacts on Scrooge\'s personal and professional life."', '"HIS OFFICE":"His office is depicted as being kept at a consistently low temperature even during the dog-days of summer."', '"BEGGARS":"Beggars" refers to individuals who seek help from others due to their inability to provide for themselves. In the context provided, there seems to be a discrepancy in Scrooge\'s behavior towards these "Beggars". Initially, it is stated that "Beggars are not able to get Scrooge\'s attention or assistance, indicating his lack of empathy towards them." This suggests that Scrooge does not show concern for the welfare of beggars and may be indifferent to their plight. However, another description contradicts this by stating that "Scrooge questions beggars, showing his interest and concern for those less fortunate." If we resolve these contradictions, it could imply that while Scrooge might initially appear uninterested or uncaring towards beggars, he does take the time to inquire about their circumstances, which suggests a level of interest and possibly empathy. Therefore, Scrooge\'s behavior towards "Beggars" seems to involve both indifference and some level of concern, depending on the context in which these interactions occur.', '"CHILDREN":Children in relation to Scrooge exhibit contrasting behaviors and perceptions. On one hand, it is suggested that Scrooge doesn\'t engage with children when they ask for assistance or information, implying he lacks concern for their needs ("Children also don\'t receive any interaction from Scrooge when asking for the time, suggesting he doesn\'t care about their needs either."). Conversely, there\'s a depiction of Scrooge interacting with children and finding pleasure in their presence and innocence ("Scrooge interacts with children, finding pleasure in their presence and innocence"). \n\nThe Children themselves are portrayed in different ways. They can be seen as wretched, abject, frightful, and miserable figures, symbolizing the degradation and perversion of humanity ("The Children are depicted as wretched, abject, frightful, and miserable figures, representing the degradation and perversion of humanity"). However, there\'s also a scene where children are running out into the snow to meet their married sisters, brothers, cousins, uncles, and aunts ("The children are running out into the snow to meet their married sisters, brothers, cousins, uncles, aunts").\n\nIn summary, Scrooge\'s interaction with children varies significantly depending on the context. He may either ignore them or find pleasure in their company. The Children themselves can be seen as symbols of human degradation or ordinary individuals enjoying social gatherings.\n\nPlease note that without more specific context about these descriptions and how they relate to each other (e.g., are they from different literary works, historical periods, etc.), it\'s challenging to provide a definitive interpretation or resolution of contradictions.', '"MAN AND WOMAN":"Neither men nor women are able to get directions from Scrooge, indicating his lack of social engagement or concern."']}
18:51:20,142 datashaper.workflow.workflow ERROR Error executing verb "text_embed" in create_final_entities: Error code: 400 - {'error': {'message': 'json: cannot unmarshal array into Go struct field Message.messages.content of type string', 'type': 'invalid_request_error', 'param': None, 'code': None}}
Traceback (most recent call last):
  File "/home/pcuser/anaconda3/envs/graphrag/lib/python3.11/site-packages/datashaper/workflow/workflow.py", line 415, in _execute_verb
    result = await result
             ^^^^^^^^^^^^
  File "/home/pcuser/anaconda3/envs/graphrag/lib/python3.11/site-packages/graphrag/index/verbs/text/embed/text_embed.py", line 105, in text_embed
    return await _text_embed_in_memory(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/pcuser/anaconda3/envs/graphrag/lib/python3.11/site-packages/graphrag/index/verbs/text/embed/text_embed.py", line 130, in _text_embed_in_memory
    result = await strategy_exec(texts, callbacks, cache, strategy_args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/pcuser/anaconda3/envs/graphrag/lib/python3.11/site-packages/graphrag/index/verbs/text/embed/strategies/openai.py", line 61, in run
    embeddings = await _execute(llm, text_batches, ticker, semaphore)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/pcuser/anaconda3/envs/graphrag/lib/python3.11/site-packages/graphrag/index/verbs/text/embed/strategies/openai.py", line 105, in _execute
    results = await asyncio.gather(*futures)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/pcuser/anaconda3/envs/graphrag/lib/python3.11/site-packages/graphrag/index/verbs/text/embed/strategies/openai.py", line 99, in embed
    chunk_embeddings = await llm(chunk)
                       ^^^^^^^^^^^^^^^^
  File "/home/pcuser/anaconda3/envs/graphrag/lib/python3.11/site-packages/graphrag/llm/openai/json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/pcuser/anaconda3/envs/graphrag/lib/python3.11/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/pcuser/anaconda3/envs/graphrag/lib/python3.11/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/pcuser/anaconda3/envs/graphrag/lib/python3.11/site-packages/graphrag/llm/base/caching_llm.py", line 104, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/pcuser/anaconda3/envs/graphrag/lib/python3.11/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/pcuser/anaconda3/envs/graphrag/lib/python3.11/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "/home/pcuser/anaconda3/envs/graphrag/lib/python3.11/site-packages/tenacity/asyncio/__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/pcuser/anaconda3/envs/graphrag/lib/python3.11/site-packages/tenacity/asyncio/__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/pcuser/anaconda3/envs/graphrag/lib/python3.11/site-packages/tenacity/_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/pcuser/anaconda3/envs/graphrag/lib/python3.11/site-packages/tenacity/__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "/home/pcuser/anaconda3/envs/graphrag/lib/python3.11/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/home/pcuser/anaconda3/envs/graphrag/lib/python3.11/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/pcuser/anaconda3/envs/graphrag/lib/python3.11/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "/home/pcuser/anaconda3/envs/graphrag/lib/python3.11/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/pcuser/anaconda3/envs/graphrag/lib/python3.11/site-packages/graphrag/llm/base/base_llm.py", line 49, in __call__
    return await self._invoke(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/pcuser/anaconda3/envs/graphrag/lib/python3.11/site-packages/graphrag/llm/base/base_llm.py", line 53, in _invoke
    output = await self._execute_llm(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/pcuser/anaconda3/envs/graphrag/lib/python3.11/site-packages/graphrag/llm/openai/openai_chat_llm.py", line 55, in _execute_llm
    completion = await self.client.chat.completions.create(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/pcuser/anaconda3/envs/graphrag/lib/python3.11/site-packages/openai/resources/chat/completions.py", line 1295, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "/home/pcuser/anaconda3/envs/graphrag/lib/python3.11/site-packages/openai/_base_client.py", line 1826, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/pcuser/anaconda3/envs/graphrag/lib/python3.11/site-packages/openai/_base_client.py", line 1519, in request
    return await self._request(
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/pcuser/anaconda3/envs/graphrag/lib/python3.11/site-packages/openai/_base_client.py", line 1620, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': 'json: cannot unmarshal array into Go struct field Message.messages.content of type string', 'type': 'invalid_request_error', 'param': None, 'code': None}}
18:51:20,147 graphrag.index.reporting.file_workflow_callbacks INFO Error executing verb "text_embed" in create_final_entities: Error code: 400 - {'error': {'message': 'json: cannot unmarshal array into Go struct field Message.messages.content of type string', 'type': 'invalid_request_error', 'param': None, 'code': None}} details=None
18:51:20,147 graphrag.index.run ERROR error running workflow create_final_entities
Traceback (most recent call last):
  File "/home/pcuser/anaconda3/envs/graphrag/lib/python3.11/site-packages/graphrag/index/run.py", line 323, in run_pipeline
    result = await workflow.run(context, callbacks)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/pcuser/anaconda3/envs/graphrag/lib/python3.11/site-packages/datashaper/workflow/workflow.py", line 369, in run
    timing = await self._execute_verb(node, context, callbacks)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/pcuser/anaconda3/envs/graphrag/lib/python3.11/site-packages/datashaper/workflow/workflow.py", line 415, in _execute_verb
    result = await result
             ^^^^^^^^^^^^
  File "/home/pcuser/anaconda3/envs/graphrag/lib/python3.11/site-packages/graphrag/index/verbs/text/embed/text_embed.py", line 105, in text_embed
    return await _text_embed_in_memory(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/pcuser/anaconda3/envs/graphrag/lib/python3.11/site-packages/graphrag/index/verbs/text/embed/text_embed.py", line 130, in _text_embed_in_memory
    result = await strategy_exec(texts, callbacks, cache, strategy_args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/pcuser/anaconda3/envs/graphrag/lib/python3.11/site-packages/graphrag/index/verbs/text/embed/strategies/openai.py", line 61, in run
    embeddings = await _execute(llm, text_batches, ticker, semaphore)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/pcuser/anaconda3/envs/graphrag/lib/python3.11/site-packages/graphrag/index/verbs/text/embed/strategies/openai.py", line 105, in _execute
    results = await asyncio.gather(*futures)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/pcuser/anaconda3/envs/graphrag/lib/python3.11/site-packages/graphrag/index/verbs/text/embed/strategies/openai.py", line 99, in embed
    chunk_embeddings = await llm(chunk)
                       ^^^^^^^^^^^^^^^^
  File "/home/pcuser/anaconda3/envs/graphrag/lib/python3.11/site-packages/graphrag/llm/openai/json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/pcuser/anaconda3/envs/graphrag/lib/python3.11/site-packages/graphrag/llm/openai/openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/pcuser/anaconda3/envs/graphrag/lib/python3.11/site-packages/graphrag/llm/openai/openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/pcuser/anaconda3/envs/graphrag/lib/python3.11/site-packages/graphrag/llm/base/caching_llm.py", line 104, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/pcuser/anaconda3/envs/graphrag/lib/python3.11/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/pcuser/anaconda3/envs/graphrag/lib/python3.11/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "/home/pcuser/anaconda3/envs/graphrag/lib/python3.11/site-packages/tenacity/asyncio/__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/pcuser/anaconda3/envs/graphrag/lib/python3.11/site-packages/tenacity/asyncio/__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/pcuser/anaconda3/envs/graphrag/lib/python3.11/site-packages/tenacity/_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/pcuser/anaconda3/envs/graphrag/lib/python3.11/site-packages/tenacity/__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "/home/pcuser/anaconda3/envs/graphrag/lib/python3.11/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/home/pcuser/anaconda3/envs/graphrag/lib/python3.11/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/home/pcuser/anaconda3/envs/graphrag/lib/python3.11/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "/home/pcuser/anaconda3/envs/graphrag/lib/python3.11/site-packages/graphrag/llm/base/rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/pcuser/anaconda3/envs/graphrag/lib/python3.11/site-packages/graphrag/llm/base/base_llm.py", line 49, in __call__
    return await self._invoke(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/pcuser/anaconda3/envs/graphrag/lib/python3.11/site-packages/graphrag/llm/base/base_llm.py", line 53, in _invoke
    output = await self._execute_llm(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/pcuser/anaconda3/envs/graphrag/lib/python3.11/site-packages/graphrag/llm/openai/openai_chat_llm.py", line 55, in _execute_llm
    completion = await self.client.chat.completions.create(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/pcuser/anaconda3/envs/graphrag/lib/python3.11/site-packages/openai/resources/chat/completions.py", line 1295, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "/home/pcuser/anaconda3/envs/graphrag/lib/python3.11/site-packages/openai/_base_client.py", line 1826, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/pcuser/anaconda3/envs/graphrag/lib/python3.11/site-packages/openai/_base_client.py", line 1519, in request
    return await self._request(
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/pcuser/anaconda3/envs/graphrag/lib/python3.11/site-packages/openai/_base_client.py", line 1620, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': 'json: cannot unmarshal array into Go struct field Message.messages.content of type string', 'type': 'invalid_request_error', 'param': None, 'code': None}}
18:51:20,149 graphrag.index.reporting.file_workflow_callbacks INFO Error running pipeline! details=None
